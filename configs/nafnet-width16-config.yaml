name: NAFNet-width16
model_type: ImageRestorationModel
manual_seed: 42

datasets:
  # this params is same as the keywords args taken by dataset class to instatiate
  dataset_dir: Dataset
  target_size: [256,256,3]
  batch_size: 8
  input_folder_name: blur
  target_folder_name: sharp
  validation_split: 0.1
  # supported augmentations ["random_crop", "flip_left_right", "flip_up_down", "random_contrast", "random_saturation", "random_brightness", "random_hue", "random_jpeg_quality"]
  augmenting_list: ["random_crop", "flip_left_right", "flip_up_down"]
  augment_target_images: True
  cache_train_dataset: True
  cache_valid_dataset: True

model:
  name: NAFnet
  model_params:
    # this model configuration is same as all the keywords args a model class takes
    # to instatiate
    width: 32
    num_enc_blocks: [1, 1, 1, 16]
    num_middle_blocks: 1
    num_dec_blocks: [1, 1, 1, 1]
    train_size: [256, 256, 3]
    dropout_rate: 0.1
    local_agg: True #if set to True, then set save_only_weights to True.
    tlc_factor: 1.5

# training settings
train:
  epoch: 2000

  optimizer:
    name: Adam
    weight_decay: ~
    beta_1: 0.9
    beta_2: 0.9
    clipvalue: 0.0
    clipnorm: ~
    global_clipnorm: ~
    amsgrad: False
    use_ema: False
    ema_momentum: 0.99
    jit_compile: True

  scheduler:
    name: CosineDecay # ["CosineDecayRestarts", "CosineDecay","ExponentialDecay", "InverseTimeDecay","PolynomialDecay"]
    initial_learning_rate: !!float 1e-3
    decay_steps: 60000
    # t_mul: 2.0
    # m_mul: 0.6
    alpha: 0.005
  
  losses:
    psnr_loss: 
      weight: 1.0
    # charbonnier_loss:
    #   weight: 2.0
  
  metrics: ["psnr","ssim"]

  pretrained_path: ~

model_save:
  frequency: 2
  save_format: tf
  save_only_weights: True
  checkpoint_dir: checkpoints
  saved_model_dir: trained_model

tb_logger:
  log_dir: logs/
  log_image: True